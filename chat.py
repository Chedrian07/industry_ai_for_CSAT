from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

# ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
print("ëª¨ë¸ ë¡œë”© ì¤‘...")
base_model = AutoModelForCausalLM.from_pretrained(
    "LGAI-EXAONE/EXAONE-4.0-1.2B",
    torch_dtype=torch.float32,
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained("LGAI-EXAONE/EXAONE-4.0-1.2B")

# LoRA ëª¨ë¸ ë¡œë“œ
print("LoRA ëª¨ë¸ ë¡œë”© ì¤‘...")
model = PeftModel.from_pretrained(base_model, "./final_model")

def generate_with_reasoning(question, enable_reasoning=True):
    """ì¶”ë¡  ëª¨ë“œë¡œ ë‹µë³€ ìƒì„±"""
    messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚°ì—…ê¸°ìˆ  ë¶„ì•¼ì˜ ì „ë¬¸ê°€ë¡œì„œ ê°ê´€ì‹ ë¬¸ì œë¥¼ ë¶„ì„í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” êµìœ¡ ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
        {"role": "user", "content": question}
    ]
    
    # EXAONE 4.0 ì¶”ë¡  ëª¨ë“œ í™œì„±í™”
    input_ids = tokenizer.apply_chat_template(
        messages, 
        tokenize=True, 
        add_generation_prompt=True, 
        return_tensors="pt",
        enable_thinking=enable_reasoning  # ì¶”ë¡  ëª¨ë“œ í™œì„±í™”
    )
    
    # ì¶”ë¡ ì„ ìœ„í•œ ìƒì„± ì„¤ì •
    generation_config = {
        "max_new_tokens": 1024,  # ì¶”ë¡ ì„ ìœ„í•´ ì¶©ë¶„í•œ í† í°
        "temperature": 0.6,      # EXAONE ê¶Œì¥ ì„¤ì •
        "top_p": 0.95,          # EXAONE ê¶Œì¥ ì„¤ì •
        "do_sample": True,
        "eos_token_id": tokenizer.eos_token_id,
        "pad_token_id": tokenizer.pad_token_id,
    }
    
    print("ë‹µë³€ ìƒì„± ì¤‘...")
    with torch.no_grad():
        output = model.generate(
            input_ids.to(model.device),
            **generation_config
        )
    
    # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
    response = tokenizer.decode(output[0], skip_special_tokens=False)
    
    # ì¶”ë¡  ë¶€ë¶„ê³¼ ìµœì¢… ë‹µë³€ ë¶„ë¦¬
    if enable_reasoning and "<think>" in response and "</think>" in response:
        thinking_start = response.find("<think>")
        thinking_end = response.find("</think>") + 8
        thinking_part = response[thinking_start:thinking_end]
        final_answer = response[thinking_end:].strip()
        
        print("="*50)
        print("ğŸ§  ì¶”ë¡  ê³¼ì •:")
        print("="*50)
        print(thinking_part)
        print("\n" + "="*50)
        print("ğŸ“ ìµœì¢… ë‹µë³€:")
        print("="*50)
        print(final_answer)
    else:
        print("="*50)
        print("ğŸ“ ë‹µë³€:")
        print("="*50)
        print(response)
    
    return response

def interactive_chat():
    """ëŒ€í™”í˜• ì±„íŒ…"""
    print("="*60)
    print("ğŸ“ EXAONE 4.0 ì‚°ì—…ê¸°ìˆ  ì „ë¬¸ê°€ (ì¶”ë¡  ëª¨ë“œ)")
    print("="*60)
    print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit')")
    print("ì¶”ë¡  ëª¨ë“œ ì „í™˜: 'reasoning on/off'")
    print("-"*60)
    
    enable_reasoning = True
    
    while True:
        user_input = input("\nâ“ ì§ˆë¬¸: ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        elif user_input.lower() == 'reasoning on':
            enable_reasoning = True
            print("ğŸ§  ì¶”ë¡  ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            continue
        elif user_input.lower() == 'reasoning off':
            enable_reasoning = False
            print("ğŸ’¬ ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
            continue
        elif not user_input:
            continue
        
        try:
            generate_with_reasoning(user_input, enable_reasoning)
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸ”§ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ìœ¼ë¡œ ì‹œì‘...")
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_question = """ë‹¤ìŒ ì¤‘ ì‹í’ˆ ê³µì—…ì˜ íŠ¹ì§•ìœ¼ë¡œ ê°€ì¥ ì ì ˆí•œ ê²ƒì€?
    
â‘  ëŒ€ëŸ‰ ìƒì‚°ì´ ì£¼ë¥¼ ì´ë£¬ë‹¤
â‘¡ ê³„ì ˆì  ë³€ë™ì´ ì ë‹¤  
â‘¢ ì›ë£Œì˜ ë¶€íŒ¨ì„±ì´ ë†’ë‹¤
â‘£ ìë™í™” ìˆ˜ì¤€ì´ ë‚®ë‹¤
â‘¤ í‘œì¤€í™”ê°€ ì–´ë µë‹¤"""
    
    generate_with_reasoning(test_question, enable_reasoning=True)
    
    # ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘
    print("\n" + "="*60)
    interactive_chat()
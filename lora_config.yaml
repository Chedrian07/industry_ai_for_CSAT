model: "LGAI-EXAONE/EXAONE-4.0-1.2B"
train: true
data: "dataset"

# ---------- LoRA (보수적) ----------
fine_tune_type: "lora"
num_layers: 24
rank: 4                   # 낮게 유지
alpha: 8                  
dropout: 0.15             # 0.1 → 0.15 (과적합 방지)
target_modules:
  - q_proj
  - v_proj
  - gate_proj
  - down_proj

# ---------- 학습 ----------
batch_size: 8             
grad_accum: 16            # Effective BS = 128
iters: 750                # 10 epochs (74 steps/epoch)
learning_rate: 2e-5       
weight_decay: 0.02        # 0.01 → 0.02 (강화)
max_seq_length: 2048
grad_checkpoint: true
gradient_clip: 1.0        

# ---------- LR 스케줄 ----------
lr_schedule:
  name: cosine_decay
  warmup: 75              # 1 epoch
  warmup_init: 1e-7
  arguments: [2e-5, 750, 1e-7] 

early_stop_threshold: 2

# ---------- 로깅 ----------
steps_per_report: 10
steps_per_eval: 25        
save_every: 75            # 1 epoch마다
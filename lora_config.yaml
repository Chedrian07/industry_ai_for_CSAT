# === QLoRA for EXAONE-3.5-7.8B (4-bit) on M1 Max 32 GB ===
#model: "./mlx_model"  # ① 위 convert 결과 경로 또는 HF 4-bit repo
# model: LGAI-EXAONE/EXAONE-4.0-1.2b
# train: true
# data: "dataset"                            # ② train.jsonl·valid.jsonl 폴더

# # ---------- LoRA / QLoRA ----------
# fine_tune_type: "lora"
# num_layers: 30            # ← 수정 ①
# rank: 8
# alpha: 16
# dropout: 0.05
# target_modules:
#   - q_proj
#   - k_proj
#   - v_proj
#   - o_proj
#   - gate_proj
#   - up_proj
#   - down_proj

# # ---------- 학습 ----------
# batch_size: 4
# grad_accum: 16             # ⇒ Effective BS 64   ← 수정 ②(주석)
# iters: 450                 # ≈ 3 epoch          ← 수정 ④
# learning_rate: 3e-5        # ← 수정 ③
# max_seq_length: 4096
# grad_checkpoint: true

# # ---------- LR 스케줄 ----------
# lr_schedule:
#   name: cosine_decay
#   warmup: 23
#   warmup_init: 1e-7
#   arguments: [3e-5, 450, 1e-6]  # ← base_lr 일치

# # ---------- 로깅 ----------
# steps_per_report: 50
# steps_per_eval: 75
# save_every: 150
# seed: 42

# === QLoRA for EXAONE-3.5-7.8B (4-bit) ===
# model: "./mlx_model"      # ① 위 convert 결과 경로 또는 HF 4-bit repo
# === QLoRA for EXAONE-3.5-7.8B (4-bit) ===

# === QLoRA for EXAONE-3.5-7.8B (4-bit) on M1 Max 32 GB ===

# model: "mlx_model"
# train: true
# data: "dataset"

# # ---------- LoRA / QLoRA ----------
# fine_tune_type: "lora"
# num_layers: 32            # 모델에 맞게 확인 필요
# rank: 16                  # 강한 학습을 위해 증가
# alpha: 32                 
# dropout: 0.05
# target_modules:
#   - q_proj
#   - k_proj
#   - v_proj
#   - o_proj
#   - gate_proj
#   - up_proj
#   - down_proj

# # ---------- 학습 ----------
# batch_size: 6             # 요청사항 고정
# grad_accum: 16            # Effective BS = 96
# iters: 1000               # ≈ 10 epochs (98 steps/epoch)
# learning_rate: 5e-5       # 공격적 학습률
# max_seq_length: 1024      # 평균 600 토큰 고려, 메모리 효율
# grad_checkpoint: true

# # ---------- LR 스케줄 ----------
# lr_schedule:
#   name: cosine_decay
#   warmup: 50              # 첫 0.5 epoch
#   warmup_init: 1e-7
#   arguments: [5e-5, 1000, 1e-6]

# # ---------- 로깅 ----------
# steps_per_report: 10      # 자주 확인 (98 steps/epoch)
# steps_per_eval: 50        # 0.5 epoch마다
# save_every: 100           # 1 epoch마다
# seed: 42

# === QLoRA for EXAONE-4.0-1.2B (4-bit) ===

# === QLoRA for EXAONE-4.0-1.2B (10 epochs) ===

model: "LGAI-EXAONE/EXAONE-4.0-1.2B"
train: true
data: "dataset"

# ---------- LoRA (보수적) ----------
fine_tune_type: "lora"
num_layers: 24
rank: 4                   # 낮게 유지
alpha: 8                  
dropout: 0.15             # ✅ 0.1 → 0.15 (과적합 방지)
target_modules:
  - q_proj
  - v_proj
  - gate_proj
  - down_proj

# ---------- 학습 ----------
batch_size: 8             
grad_accum: 16            # Effective BS = 128
iters: 750                # ✅ 10 epochs (74 steps/epoch)
learning_rate: 2e-5       # ✅ 5e-5 → 3e-5 (더 보수적)
weight_decay: 0.02        # ✅ 0.01 → 0.02 (강화)
max_seq_length: 2048
grad_checkpoint: true
gradient_clip: 1.0        

# ---------- LR 스케줄 ----------
lr_schedule:
  name: cosine_decay
  warmup: 75              # 1 epoch
  warmup_init: 1e-7
  arguments: [2e-5, 750, 1e-7]  # ✅ 더 낮은 final LR

early_stop_threshold: 2

# ---------- 로깅 ----------
steps_per_report: 10
steps_per_eval: 25        
save_every: 75            # 1 epoch마다
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CUDA í™˜ê²½ì—ì„œ EXAONE-4.0-1.2B ëª¨ë¸ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
Transformers + LoRA ì¡°í•© ì‚¬ìš©
"""

import os
import json
import time
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Any
import logging

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    get_linear_schedule_with_warmup
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training
)
import numpy as np
from tqdm import tqdm

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ChatDataset(Dataset):
    """ì±„íŒ… ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, data: List[Dict], tokenizer, max_length: int = 2048):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        messages = item.get("messages", [])
        text = self.format_chat_messages(messages)
        
        # í† í¬ë‚˜ì´ì§•
        encodings = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding=False,
            return_tensors="pt"
        )
        
        input_ids = encodings["input_ids"].squeeze()
        
        return {
            "input_ids": input_ids,
            "labels": input_ids.clone()  # ì–¸ì–´ ëª¨ë¸ë§ì—ì„œëŠ” inputê³¼ labelì´ ë™ì¼
        }
    
    def format_chat_messages(self, messages: List[Dict]) -> str:
        """ì±„íŒ… ë©”ì‹œì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        formatted_text = ""
        
        for message in messages:
            role = message.get("role", "")
            content = message.get("content", "")
            
            if role == "system":
                formatted_text += f"<|system|>\n{content}\n"
            elif role == "user":
                formatted_text += f"<|user|>\n{content}\n"
            elif role == "assistant":
                formatted_text += f"<|assistant|>\n{content}\n"
        
        formatted_text += "<|endoftext|>"
        return formatted_text

class ExaoneTrainer:
    """EXAONE ëª¨ë¸ í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ì‹œë“œ ì„¤ì •
        torch.manual_seed(args.seed)
        torch.cuda.manual_seed_all(args.seed)
        np.random.seed(args.seed)
        
        logger.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            logger.info(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
            logger.info(f"GPU ì´ë¦„: {torch.cuda.get_device_name()}")
    
    def load_model_and_tokenizer(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        logger.info(f"ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘: {self.args.model_name}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.args.model_name,
            trust_remote_code=True,
            use_fast=False
        )
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForCausalLM.from_pretrained(
            self.args.model_name,
            torch_dtype=torch.float16 if self.args.use_fp16 else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            use_cache=False  # í›ˆë ¨ ì‹œ ìºì‹œ ë¹„í™œì„±í™”
        )
        
        # ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
        self.model.train()
        
        logger.info("âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ!")
        logger.info(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in self.model.parameters()):,}")
    
    def setup_lora(self):
        """LoRA ì„¤ì •ì„ ì ìš©í•©ë‹ˆë‹¤."""
        logger.info("LoRA ì„¤ì • ì ìš© ì¤‘...")
        
        # LoRA ì„¤ì •
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=self.args.lora_rank,
            lora_alpha=self.args.lora_alpha,
            lora_dropout=self.args.lora_dropout,
            target_modules=[
                "q_proj",
                "k_proj", 
                "v_proj",
                "o_proj",
                "gate_proj",
                "up_proj",
                "down_proj"
            ],
            bias="none",
        )
        
        # ëª¨ë¸ì„ LoRAë¡œ ì¤€ë¹„
        if self.args.use_fp16:
            self.model = prepare_model_for_kbit_training(self.model)
        
        self.model = get_peft_model(self.model, lora_config)
        
        # í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        
        logger.info(f"í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
        logger.info(f"ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
        logger.info(f"í›ˆë ¨ ê°€ëŠ¥í•œ ë¹„ìœ¨: {100 * trainable_params / total_params:.2f}%")
        logger.info("âœ… LoRA ì„¤ì • ì™„ë£Œ!")
    
    def load_data(self):
        """ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        logger.info("ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
        
        # í›ˆë ¨ ë°ì´í„° ë¡œë“œ
        train_data = self.load_jsonl_data(self.args.train_data_path)
        valid_data = self.load_jsonl_data(self.args.valid_data_path)
        
        logger.info(f"í›ˆë ¨ ë°ì´í„°: {len(train_data)}ê°œ")
        logger.info(f"ê²€ì¦ ë°ì´í„°: {len(valid_data)}ê°œ")
        
        if len(train_data) == 0:
            raise ValueError("í›ˆë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        
        # ë°ì´í„°ì…‹ ìƒì„±
        self.train_dataset = ChatDataset(train_data, self.tokenizer, self.args.max_sequence_length)
        self.valid_dataset = ChatDataset(valid_data, self.tokenizer, self.args.max_sequence_length) if valid_data else None
        
        logger.info("âœ… ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ!")
    
    def load_jsonl_data(self, file_path: str) -> List[Dict]:
        """JSONL íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        data = []
        if not os.path.exists(file_path):
            return data
            
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line.strip()))
        return data
    
    def create_data_collator(self):
        """ë°ì´í„° ì½œë ˆì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # Causal LMì´ë¯€ë¡œ False
            pad_to_multiple_of=8 if self.args.use_fp16 else None,
        )
    
    def train(self):
        """í›ˆë ¨ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        logger.info("í›ˆë ¨ ì‹œì‘!")
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.load_model_and_tokenizer()
        
        # LoRA ì„¤ì •
        if self.args.use_lora:
            self.setup_lora()
        
        # ë°ì´í„° ë¡œë“œ
        self.load_data()
        
        # ë°ì´í„° ì½œë ˆì´í„° ìƒì„±
        data_collator = self.create_data_collator()
        
        # í›ˆë ¨ ì¸ìˆ˜ ì„¤ì •
        training_args = TrainingArguments(
            output_dir=self.args.output_dir,
            overwrite_output_dir=True,
            num_train_epochs=self.args.num_epochs,
            per_device_train_batch_size=self.args.batch_size,
            per_device_eval_batch_size=self.args.batch_size,
            gradient_accumulation_steps=self.args.gradient_accumulation_steps,
            learning_rate=self.args.learning_rate,
            weight_decay=self.args.weight_decay,
            warmup_steps=self.args.warmup_steps,
            logging_steps=10,
            logging_dir=f"{self.args.output_dir}/logs",
            evaluation_strategy="epoch" if self.valid_dataset else "no",
            save_strategy="epoch",
            save_total_limit=3,
            load_best_model_at_end=True if self.valid_dataset else False,
            metric_for_best_model="eval_loss" if self.valid_dataset else None,
            greater_is_better=False,
            report_to=None,  # wandb ë“± ë¹„í™œì„±í™”
            dataloader_pin_memory=True,
            gradient_checkpointing=self.args.gradient_checkpointing,
            fp16=self.args.use_fp16,
            dataloader_num_workers=4,
            remove_unused_columns=False,
            max_grad_norm=self.args.max_grad_norm,
        )
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.valid_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer,
        )
        
        # í›ˆë ¨ ì‹œì‘
        logger.info("ğŸš€ í›ˆë ¨ ì‹œì‘!")
        trainer.train()
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_output_dir = os.path.join(self.args.output_dir, "final_model")
        trainer.save_model(final_output_dir)
        self.tokenizer.save_pretrained(final_output_dir)
        
        logger.info(f"ğŸ‰ í›ˆë ¨ ì™„ë£Œ! ëª¨ë¸ì´ {final_output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    parser = argparse.ArgumentParser(description="EXAONE-4.0-1.2B Fine-tuning with CUDA")
    
    # ëª¨ë¸ ê´€ë ¨ ì„¤ì •
    parser.add_argument("--model_name", type=str, 
                       default="LGAI-EXAONE/EXAONE-4.0-1.2B",
                       help="ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ")
    
    # ë°ì´í„° ê´€ë ¨ ì„¤ì •
    parser.add_argument("--train_data_path", type=str, 
                       default="./dataset/train.jsonl",
                       help="í›ˆë ¨ ë°ì´í„° ê²½ë¡œ")
    parser.add_argument("--valid_data_path", type=str, 
                       default="./dataset/valid.jsonl",
                       help="ê²€ì¦ ë°ì´í„° ê²½ë¡œ")
    
    # í›ˆë ¨ ê´€ë ¨ ì„¤ì •
    parser.add_argument("--batch_size", type=int, default=4,
                       help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8,
                       help="ê¸°ìš¸ê¸° ëˆ„ì  ìŠ¤í… ìˆ˜")
    parser.add_argument("--max_sequence_length", type=int, default=2048,
                       help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´")
    parser.add_argument("--num_epochs", type=int, default=3,
                       help="í›ˆë ¨ ì—í­ ìˆ˜")
    parser.add_argument("--learning_rate", type=float, default=2e-5,
                       help="í•™ìŠµë¥ ")
    parser.add_argument("--weight_decay", type=float, default=0.01,
                       help="ê°€ì¤‘ì¹˜ ê°ì‡ ")
    parser.add_argument("--warmup_steps", type=int, default=100,
                       help="ì›œì—… ìŠ¤í… ìˆ˜")
    parser.add_argument("--max_grad_norm", type=float, default=1.0,
                       help="ê¸°ìš¸ê¸° í´ë¦¬í•‘ ì„ê³„ê°’")
    
    # LoRA ê´€ë ¨ ì„¤ì •
    parser.add_argument("--use_lora", action="store_true",
                       help="LoRA ì‚¬ìš© ì—¬ë¶€")
    parser.add_argument("--lora_rank", type=int, default=64,
                       help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=16,
                       help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.1,
                       help="LoRA dropout")
    
    # ìµœì í™” ê´€ë ¨ ì„¤ì •
    parser.add_argument("--use_fp16", action="store_true",
                       help="FP16 í˜¼í•© ì •ë°€ë„ ì‚¬ìš©")
    parser.add_argument("--gradient_checkpointing", action="store_true",
                       help="ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)")
    
    # ì¶œë ¥ ê´€ë ¨ ì„¤ì •
    parser.add_argument("--output_dir", type=str, default="./output",
                       help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--seed", type=int, default=42,
                       help="ëœë¤ ì‹œë“œ")
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ì„¤ì • ì¶œë ¥
    logger.info("=" * 50)
    logger.info("EXAONE-4.0-1.2B CUDA Fine-tuning")
    logger.info("=" * 50)
    logger.info(f"ëª¨ë¸: {args.model_name}")
    logger.info(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    logger.info(f"ê¸°ìš¸ê¸° ëˆ„ì : {args.gradient_accumulation_steps}")
    logger.info(f"ìœ íš¨ ë°°ì¹˜ í¬ê¸°: {args.batch_size * args.gradient_accumulation_steps}")
    logger.info(f"í•™ìŠµë¥ : {args.learning_rate}")
    logger.info(f"ì—í­ ìˆ˜: {args.num_epochs}")
    logger.info(f"ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {args.max_sequence_length}")
    logger.info(f"LoRA ì‚¬ìš©: {args.use_lora}")
    if args.use_lora:
        logger.info(f"LoRA rank: {args.lora_rank}")
        logger.info(f"LoRA alpha: {args.lora_alpha}")
        logger.info(f"LoRA dropout: {args.lora_dropout}")
    logger.info(f"FP16 ì‚¬ìš©: {args.use_fp16}")
    logger.info(f"ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…: {args.gradient_checkpointing}")
    logger.info("=" * 50)
    
    # í›ˆë ¨ ì‹¤í–‰
    trainer = ExaoneTrainer(args)
    trainer.train()

if __name__ == "__main__":
    main()
